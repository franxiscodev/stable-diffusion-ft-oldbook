{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Stable Diffusion v1-4 con Old Book Illustrations\n",
    "\n",
    "Adaptacion del notebook del docente (`2.finetuning_stable_diffusion.ipynb`)\n",
    "para el dataset `gigant/oldbookillustrations`.\n",
    "\n",
    "**3 cambios respecto al codigo del docente:**\n",
    "1. `example[\"image\"]` -> `example[\"1600px\"]` (columna de imagen)\n",
    "2. `example[\"text\"]` -> `example[\"info_alt\"]` (columna de caption)\n",
    "3. `Resize((512,512))` -> `Resize(512) + CenterCrop(512)` (imagenes no cuadradas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerias necesarias:\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a utilizar GPU si esta disponible, o CPU:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "# Cargamos un modelo pre-entrenado Stable Diffusion:\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    pretrained_model_name,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos una imagen ANTES del fine-tuning:\n",
    "prompt = \"an illustration of a ship sailing through a stormy sea\"\n",
    "image_before = pipe(prompt).images[0]\n",
    "\n",
    "image_before.save(\"../generated/before_finetuning.png\")\n",
    "image_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargando el dataset Old Book Illustrations:\n",
    "dataset_name = \"gigant/oldbookillustrations\"\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Comprobamos el tamano de las imagenes del dataset:\n",
    "size = dataset[0][\"1600px\"].size\n",
    "print(f\"Tamano de las imagenes del dataset: {size}\")\n",
    "print(f\"Numero de muestras: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las transformaciones necesarias para el dataset:\n",
    "# CAMBIO 3: Resize(512) + CenterCrop(512) en vez de Resize((512,512))\n",
    "# Las imagenes de Old Book Illustrations no son cuadradas,\n",
    "# asi que usamos Resize + CenterCrop para preservar proporciones.\n",
    "resolution = 512\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(resolution),                          # redimensionar lado menor a 512\n",
    "    transforms.CenterCrop(resolution),                      # recortar cuadrado central\n",
    "    transforms.ToTensor(),                                   # convertir a tensor\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # normalizacion\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la transformacion a una imagen aleatoria del dataset:\n",
    "original_image = dataset[0][\"1600px\"]\n",
    "transformed_image = image_transforms(original_image.convert(\"RGB\"))\n",
    "transformed_pil_image = transforms.ToPILImage()(transformed_image)\n",
    "\n",
    "print(\"Imagen transformada:\")\n",
    "transformed_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de cargar todas las componentes, liberamos la pipeline:\n",
    "del pipe\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizador:\n",
    "tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name, subfolder='tokenizer')\n",
    "\n",
    "# Scheduler:\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name, subfolder=\"scheduler\")\n",
    "\n",
    "# Text Encoder (CLIP):\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    subfolder=\"text_encoder\",\n",
    ").to(device)\n",
    "\n",
    "# VAE: Autoencoder:\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    subfolder=\"vae\",\n",
    ").to(device)\n",
    "\n",
    "# La UNet:\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    subfolder=\"unet\",\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Dataset wrapper para la hora del entrenamiento:\n",
    "batch_size = 6\n",
    "\n",
    "class Text2ImageDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        # CAMBIO 1: \"1600px\" en vez de \"image\"\n",
    "        image = image_transforms(example[\"1600px\"].convert(\"RGB\"))\n",
    "        # CAMBIO 2: \"info_alt\" en vez de \"text\"\n",
    "        token = tokenizer(example[\"info_alt\"], padding=\"max_length\", truncation=True, max_length=tokenizer.model_max_length, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"input_ids\": token.input_ids.squeeze(0),\n",
    "            \"attention_mask\": token.attention_mask.squeeze(0),\n",
    "        }\n",
    "\n",
    "train_dataset = Text2ImageDataset(dataset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congelamos los pesos del VAE y del Text Encoder, ya que solo queremos finetunear la UNet:\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizador:\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
    "\n",
    "# Acelerador:\n",
    "accelerator = Accelerator()\n",
    "unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)\n",
    "print(accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop:\n",
    "num_epochs = 2\n",
    "\n",
    "os.makedirs(\"../outputs/checkpoints\", exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        # Se pasan los pixeles al espacio latente con el encoder del VAE:\n",
    "        with torch.no_grad():\n",
    "            latents = vae.encode(batch[\"pixel_values\"].to(accelerator.device)).latent_dist.sample()\n",
    "            latents = latents * 0.18215\n",
    "\n",
    "        # Proceso de difusion hacia delante:\n",
    "        # 1. Creamos ruido aleatorio\n",
    "        noise = torch.randn_like(latents)\n",
    "        # 2. Cogemos un timestep aleatorio:\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device).long()\n",
    "        # 3. Anadimos ruido al vector del espacio latente:\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Codificamos el texto:\n",
    "        encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(accelerator.device))[0]\n",
    "\n",
    "        # Con el vector con ruido, el timestep, y el vector de texto, hacemos la prediccion de ruido:\n",
    "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        # Calculamos el error y actualizamos los parametros:\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Loss promedio por epoch\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss promedio: {avg_loss:.6f}\")\n",
    "\n",
    "    # Checkpoint cada epoch\n",
    "    checkpoint_path = f\"../outputs/checkpoints/checkpoint-epoch-{epoch + 1}\"\n",
    "    unet.save_pretrained(checkpoint_path)\n",
    "    print(f\"Checkpoint guardado en {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo finetuneado:\n",
    "output_dir = \"../outputs/finetuned-model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "unet.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Modelo guardado en {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia con el modelo fine-tuneado\n",
    "\n",
    "Cargamos la UNet fine-tuneada y generamos una imagen con el mismo prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la UNet finetuneada:\n",
    "finetuned_unet = UNet2DConditionModel.from_pretrained(\"../outputs/finetuned-model\")\n",
    "finetuned_unet.to(device)\n",
    "\n",
    "print('Modelo finetuneado cargado correctamente!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Cargamos el modelo pre-entrenado pero sustituyendo la UNet por la nuestra:\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    unet=finetuned_unet,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos una imagen con el modelo finetuneado (mismo prompt que antes):\n",
    "prompt = \"an illustration of a ship sailing through a stormy sea\"\n",
    "image_after = pipe(prompt).images[0]\n",
    "\n",
    "image_after.save(\"../generated/after_finetuning.png\")\n",
    "image_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparacion side-by-side\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_before = Image.open(\"../generated/before_finetuning.png\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "axes[0].imshow(image_before)\n",
    "axes[0].set_title('ANTES del fine-tuning', fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(image_after)\n",
    "axes[1].set_title('DESPUES del fine-tuning', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(f'Prompt: \"{prompt}\"', fontsize=12, style='italic')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../generated/comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparacion guardada en ../generated/comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
