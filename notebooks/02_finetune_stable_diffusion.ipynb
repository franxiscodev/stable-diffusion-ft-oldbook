{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fine-tuning Stable Diffusion v1-4 con Old Book Illustrations\n\nAdaptacion del notebook (`2.finetuning_stable_diffusion.ipynb`)\npara el dataset `gigant/oldbookillustrations`.\n\n**3 cambios:**\n1. `example[\"image\"]` -> `example[\"1600px\"]` (columna de imagen)\n2. `example[\"text\"]` -> `example[\"info_alt\"]` (columna de caption)\n3. `Resize((512,512))` -> `Resize(256) + CenterCrop(256)` (imagenes no cuadradas)\n\n**Config actual:** 1000 muestras, 256px, batch_size=4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos las librerias necesarias:\nfrom diffusers import StableDiffusionPipeline, DDPMScheduler\nfrom diffusers import UNet2DConditionModel, AutoencoderKL\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom datasets import load_dataset\nfrom accelerate import Accelerator\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image, ImageFile\nimport os\nfrom tqdm import tqdm\nimport time\nfrom datetime import datetime, timedelta\n\n# Permitir imagenes truncadas del dataset\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\ndef format_eta(seconds):\n    \"\"\"Formatea segundos a string legible.\"\"\"\n    if seconds < 0:\n        return \"N/A\"\n    h = int(seconds // 3600)\n    m = int((seconds % 3600) // 60)\n    if h > 0:\n        return f\"{h}h {m:02d}m\"\n    return f\"{m}m {int(seconds % 60):02d}s\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a utilizar GPU si esta disponible, o CPU:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "# Cargamos un modelo pre-entrenado Stable Diffusion:\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    pretrained_model_name,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generamos una imagen ANTES del fine-tuning:\nprompt = \"an illustration of a ship sailing through a stormy sea\"\nimage_before = pipe(prompt, height=256, width=256).images[0]\n\nimage_before.save(\"../generated/before_finetuning.png\")\nimage_before"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cargando el dataset Old Book Illustrations:\ndataset_name = \"gigant/oldbookillustrations\"\nmax_train_samples = 1000  # Limitar muestras para CPU\n\ndataset = load_dataset(dataset_name, split=\"train\")\nif max_train_samples:\n    dataset = dataset.select(range(max_train_samples))\n\n# Comprobamos el tamano de las imagenes del dataset:\nsize = dataset[0][\"1600px\"].size\nprint(f\"Tamano de las imagenes del dataset: {size}\")\nprint(f\"Numero de muestras: {len(dataset)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Definimos las transformaciones necesarias para el dataset:\n# Resize(256) + CenterCrop(256) en vez de Resize((512,512))\n# Las imagenes de Old Book Illustrations no son cuadradas,\n# asi que usamos Resize + CenterCrop para preservar proporciones.\nresolution = 256\nimage_transforms = transforms.Compose([\n    transforms.Resize(resolution),                          # redimensionar lado menor a 256\n    transforms.CenterCrop(resolution),                      # recortar cuadrado central\n    transforms.ToTensor(),                                   # convertir a tensor\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # normalizacion\n])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la transformacion a una imagen aleatoria del dataset:\n",
    "original_image = dataset[0][\"1600px\"]\n",
    "transformed_image = image_transforms(original_image.convert(\"RGB\"))\n",
    "transformed_pil_image = transforms.ToPILImage()(transformed_image)\n",
    "\n",
    "print(\"Imagen transformada:\")\n",
    "transformed_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de cargar todas las componentes, liberamos la pipeline:\n",
    "del pipe\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizador:\n",
    "tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name, subfolder='tokenizer')\n",
    "\n",
    "# Scheduler:\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name, subfolder=\"scheduler\")\n",
    "\n",
    "# Text Encoder (CLIP):\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    subfolder=\"text_encoder\",\n",
    ").to(device)\n",
    "\n",
    "# VAE: Autoencoder:\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    subfolder=\"vae\",\n",
    ").to(device)\n",
    "\n",
    "# La UNet:\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    subfolder=\"unet\",\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Creamos un Dataset wrapper para la hora del entrenamiento:\nbatch_size = 4\n\nclass Text2ImageDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        example = self.dataset[idx]\n        # CAMBIO 1: \"1600px\" en vez de \"image\"\n        image = image_transforms(example[\"1600px\"].convert(\"RGB\"))\n        # CAMBIO 2: \"info_alt\" en vez de \"text\"\n        token = tokenizer(example[\"info_alt\"], padding=\"max_length\", truncation=True, max_length=tokenizer.model_max_length, return_tensors=\"pt\")\n        return {\n            \"pixel_values\": image,\n            \"input_ids\": token.input_ids.squeeze(0),\n            \"attention_mask\": token.attention_mask.squeeze(0),\n        }\n\ntrain_dataset = Text2ImageDataset(dataset)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congelamos los pesos del VAE y del Text Encoder, ya que solo queremos finetunear la UNet:\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizador:\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
    "\n",
    "# Acelerador:\n",
    "accelerator = Accelerator()\n",
    "unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)\n",
    "print(accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop:\nnum_epochs = 2\n\nos.makedirs(\"../outputs/checkpoints\", exist_ok=True)\n\nbatches_per_epoch = len(train_dataloader)\ntotal_batches = num_epochs * batches_per_epoch\nprint(f\"Batches por epoch: {batches_per_epoch}, total: {total_batches}\")\n\nglobal_step = 0\nstart_time = time.time()\nlast_checkpoint_pct = 0\n\nfor epoch in range(num_epochs):\n    epoch_start = time.time()\n    epoch_losses = []\n    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n\n    for batch in progress_bar:\n        batch_start = time.time()\n\n        # Se pasan los pixeles al espacio latente con el encoder del VAE:\n        with torch.no_grad():\n            latents = vae.encode(batch[\"pixel_values\"].to(accelerator.device)).latent_dist.sample()\n            latents = latents * 0.18215\n\n        # Proceso de difusion hacia delante:\n        noise = torch.randn_like(latents)\n        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device).long()\n        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n        # Codificamos el texto:\n        encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(accelerator.device))[0]\n\n        # Prediccion de ruido:\n        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n\n        # Loss y backpropagation:\n        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n        accelerator.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        epoch_losses.append(loss.item())\n        global_step += 1\n        batch_time = time.time() - batch_start\n\n        # Progreso global\n        pct = global_step / total_batches * 100\n        elapsed = time.time() - start_time\n        avg_time_per_batch = elapsed / global_step\n        eta_seconds = avg_time_per_batch * (total_batches - global_step)\n\n        progress_bar.set_postfix(\n            loss=f\"{loss.item():.4f}\",\n            pct=f\"{pct:.0f}%\",\n            eta=format_eta(eta_seconds),\n        )\n\n        # Log detallado cada 10 batches\n        if global_step % 10 == 0 or global_step == 1:\n            print(f\"[Batch {global_step}/{total_batches}] \"\n                  f\"[Global {pct:.0f}%] \"\n                  f\"Loss: {loss.item():.4f} | \"\n                  f\"{batch_time:.1f}s/batch | \"\n                  f\"ETA: {format_eta(eta_seconds)}\")\n\n        # Checkpoint cada 10% de progreso\n        current_pct_bucket = int(pct // 10) * 10\n        if current_pct_bucket > last_checkpoint_pct and current_pct_bucket > 0:\n            last_checkpoint_pct = current_pct_bucket\n            ckpt_path = f\"../outputs/checkpoints/checkpoint-pct-{current_pct_bucket}\"\n            unet.save_pretrained(ckpt_path)\n            print(f\"--- Checkpoint {current_pct_bucket}% guardado en {ckpt_path} ---\")\n\n    # Fin de epoch\n    epoch_time = time.time() - epoch_start\n    avg_loss = sum(epoch_losses) / len(epoch_losses)\n    print(f\"Epoch {epoch + 1}/{num_epochs} completado - \"\n          f\"Loss promedio: {avg_loss:.6f} - \"\n          f\"Tiempo: {format_eta(epoch_time)}\")\n\n    # Checkpoint cada epoch\n    checkpoint_path = f\"../outputs/checkpoints/checkpoint-epoch-{epoch + 1}\"\n    unet.save_pretrained(checkpoint_path)\n    print(f\"Checkpoint epoch guardado en {checkpoint_path}\")\n\ntotal_time = time.time() - start_time\nprint(f\"\\nEntrenamiento completado en {format_eta(total_time)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo finetuneado:\n",
    "output_dir = \"../outputs/finetuned-model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "unet.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Modelo guardado en {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia con el modelo fine-tuneado\n",
    "\n",
    "Cargamos la UNet fine-tuneada y generamos una imagen con el mismo prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la UNet finetuneada:\n",
    "finetuned_unet = UNet2DConditionModel.from_pretrained(\"../outputs/finetuned-model\")\n",
    "finetuned_unet.to(device)\n",
    "\n",
    "print('Modelo finetuneado cargado correctamente!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Cargamos el modelo pre-entrenado pero sustituyendo la UNet por la nuestra:\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    unet=finetuned_unet,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generamos una imagen con el modelo finetuneado (mismo prompt que antes):\nprompt = \"an illustration of a ship sailing through a stormy sea\"\nimage_after = pipe(prompt, height=256, width=256).images[0]\n\nimage_after.save(\"../generated/after_finetuning.png\")\nimage_after"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparacion side-by-side\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_before = Image.open(\"../generated/before_finetuning.png\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "axes[0].imshow(image_before)\n",
    "axes[0].set_title('ANTES del fine-tuning', fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(image_after)\n",
    "axes[1].set_title('DESPUES del fine-tuning', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(f'Prompt: \"{prompt}\"', fontsize=12, style='italic')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../generated/comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparacion guardada en ../generated/comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}